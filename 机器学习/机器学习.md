# 									机器学习



[TOC]





## 符号定义：

m ： 训练样本的数量。

x ： 输入变量（或者说特征）。

y ： 输出变量（目标变量）。

$(x,y)$ ： 表示一个训练样本。

$(x^{(i)},y^{(i)})$ ：表示第i个样本。

h：假设函数。

n：特征值的数量。

$x^{(i)}$ 第 $i$ 个训练样本的特征值。（特征值可能有多个）

$x^{(i)}_j$ 第 $i$ 个训练样本中第 $j$ 个特征量的值。




## 一、绪论：初识机器学习



### 	1、机器学习的定义

> ​	Arthur Samuel：在没有明确设置的情况下，使计算机具有学习能力的研究领域。（相对陈旧，不正式）

>   ​	==Tom Mitchell：计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测定在T上的表现因经验E而提高。==
>
>   

### 2、机器学习学习算法的分类

​			机器学习中学习算法最主要的两类是监督学习和无监督学习。



#### 		2.1 监督学习

​		==监督学习是指，我们给算法一个数据集，其中包含了正确答案，算法的目的就是给出更多的正确答案。==

##### 2.1.1 回归问题

回归问题的输出是连续型变量，是一种定量输出（预测明天的气温是多少度）。

![](picture\回归学习.png)

此图中的问题便是监督学习中的回归问题，通过给定英尺与价格的数据集对房价进行预测（给定英尺预测多少钱）。

##### 2.1.2 分类问题

分类问题的输出是离散型变量（如+1、-1），是一种定性输出（预测明天是）。

![](picture\分类学习.png)

此图中的问题便是监督学习中的分类问题，通过给定肿瘤尺寸与是否为恶性肿瘤相对应的数据集，对肿瘤进行预测（给定尺寸的肿瘤为恶性还是良性）。



#### 2.2 无监督学习

​	==无监督学习是指数据集中没有标记（或都为一种标记），算法需要自行寻找数据中的结构。==

##### 聚类算法

​	将没有标记（或都为一种标记）的数据集分成不同的集合（簇）。

![](picture\聚类算法.png)



一个聚类算法典型的应用就是谷歌新闻：

![](picture\谷歌新闻（聚类应用）.png)

![](picture\谷歌新闻2.png)

谷歌新闻每天自动检索数以万计的新闻，并自动将他们聚合成一个个专题，每个专题由不同网站（大概率）的同类新闻组成。

##### 鸡尾酒会算法

![](picture\鸡尾酒会算法.png)

有这样一个场景，有一场两个人的鸡尾酒会，两个人在同时说话。同时有两个位置不同的麦克风，那么两个麦克风中所录的音频就会不同（距离不同音量不同），通过鸡尾酒会算法可以将通过两个音频，将两个人的声音分别提取出来，并去除其他杂音。



## 二、单变量线性回归

只含有一个特征（输入变量）的线性回归问题。（两个变量之间存在一次方函数关系，就称它们之间存在线性关系）



### 1、监督学习算法的工作原理

![](picture\单变量线性回归模型.png)

首先，我们向学习算法提供训练集（比如我们的房价训练集），学习算法的任务是输出一个==函数h（假设函数）==，假设函数（hypothesis）的作用是把房子的大小作为输入变量把它作为x的值，而它会试着输出相应房子的预测y值，==h是一个引导从x得到y的函数==。接下来要决定怎么表示这个假设函数h。假设图中的假设函数表示为：

​                                                              $h_{\theta}(x) = \theta_0 + \theta_1x$ 

（函数$h_{\theta}(x)$有时会缩写为$h(x)$）

这就意味着我们要预测的 y 是一个关于 x 的线性函数。

==这种模型被称为线性回归模型。==

这个例子是一个一元线性回归，被称为==单变量线性回归模型==。



### 2、代价函数

![](picture\模型参数.png)

![](picture\模型参数_2.png)

![](picture\模型参数_3.png)

假设有一个数据集表示房屋的面积与房价（如图3所示），若模型是一个单变量线性回归模型，表示为：

​                                                             $h_{\theta}(x) = \theta_0 + \theta_1x$ 

那么我们要做的，就是选择合适的 $\theta_0$ 和 $\theta_1$ ，是模型与所给的函数尽可能的拟合，以保证以后预测数据的准确性。

线性回归问题便成了一个最小化的问题，求合适的 $\theta_0$ 和 $\theta_1$ ，使得

​                                                             $\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$ 

的值最小，这将是我的线性回归的整体目标函数。

按照惯例，我们要定义一个代价函数：

​                                               $ J(\theta_0,\theta_1) =\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$ 

求适当的 $\theta_0$ 和 $\theta_1$ ，使得代价函数的值最小。

代价函数也被称为平方误差函数。

之所以要求误差的平方和，是因为误差平方代价函数对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好的发挥作用，但平方误差代价函数可能是解决线性回归问题最常用的手段。



![](picture\代价函数.png)

假设使 $\theta_0=0$  来简化假设函数 $h_{\theta}(x)$ ，那么它的代价函数 $J(\theta_0,\theta_1)$ 如右边所示，是一个碗状。而若 $\theta_0!=0 $ ，那么它的代价函数 $J(\theta_0,\theta_1)$ 也是一个碗状，如下图所示：

![](picture\代价函数_2.png)

通常为了方便也使用等高线图来表示：

![](picture\代价函数_3.png)



### 3、梯度下降

梯度下降是一种可以将代价函数 $J$ 最小化的算法。它不仅可以对线性回归模型的代价函数进行最小化，还被广泛应用于机器学习的其他领域，对其他函数进行最小化。

以下便是梯度下降的应用场景：

![](picture\梯度下降.png)

有这样一个函数 $J(\theta_0,\theta_1)$ （泛指的函数，不一定非要是代价函数），想要求得函数 $J(\theta_0,\theta_1)$ 的最小值。

然后便是梯度下降的步骤，梯度下降以一个初始值为开始，一般将 $\theta_0 = 0 , \theta_1 = 0$ ，然后对 $\theta_0,\theta_1$ 来进行变化以减少函数 $J$ 的值，不断进行这一步直到收敛到了最小值或局部最小值。

下面表示梯度下降的过程：

![](picture\梯度下降_2.png)

![](picture\梯度下降_3.png)

如图所示，梯度下降有这样的一个特点，一开始选取不同的初始值，可能最后收敛于不同的局部最小值。



![](picture\梯度下降_4.png)

梯度下降的具体算法如图所示：

​                                                   $\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)  \quad(for \quad j=0 \quad and \quad j=1)$ 

其中 $:=$ 的意思就是平常编程语言中的等号，是一个赋值符号，而 = 就是日常中的判断符号。

 $\alpha$ 称为学习速率，用来控制以多大的幅度更新这个参数 $\theta_j$ （梯度下降的速度）， $\alpha $ 的值越大，梯度下降的速度越快。

其中这个更新是对$\theta_0$ 和 $\theta_1$ 两个参数都要更新，而且是要同时更新，如图中下方左边所示。右边的不是同时更新，在某些数据集或者模型中不一定错误，但已经称不上梯度下降算法了。   

下面直观的了解下偏导项 $\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)$:

假设 $\theta_0 = 0$ ，那么代价函数 $J$ 的函数如下图所示： 

![](picture\梯度下降_5.png)

那么偏导项就变成了 $\frac{\partial}{\partial\theta_1}J(\theta_1)$ ，而偏导项其实就是函数在 $\theta_1$ 那一点的斜率，假设 $\theta_1$ 这一点在最低点的右边，那么导数的值是一个正数，也就是一个正斜率，那么根据公式 $\theta_1:=\theta_j-\alpha\frac{\partial}{\partial\theta_1}J(\theta_1)$ ，$\theta_1$ 会向左移动，正是最低点的方向。同理若是在最低点的左边，那么偏导项是一个负数，也就是一个负斜率，那么根据公式 $\theta_1$ 会向右移动，也正是最低点的方向。

![](picture\梯度下降_6.png)

而 $\alpha$ 的作用就是控制每次下降的幅度，若 $\alpha$ 数值过小，那么下降的速度欧就会很慢。而若 $\alpha$ 的数值过大，那么最坏的情况就是可能会无法收敛甚至发散。

![](picture\梯度下降_7.png)

而若到了最低点，因为最低点的导数为 0 ，所以 $\theta_1 = \theta_1-\alpha0 = 0$ ，也就是不会变化。 

![](picture\梯度下降_8.png)

随着梯度的下降，斜率不断减小，下降的速度也会不断减小。



下面将梯度下降应用到之前所学的线性回归模型中的代价函数中：

![](picture\梯度下降_9.png)

那么重点要求的就是偏导项，学过高数就可以轻松推出下图求导后的公式：

![](picture\梯度下降_10.png)



![](picture\梯度下降_11.png)

本节讲的梯度下降算法因为每一步都使用了所有的数据集，所以又称为 Batch梯度下降算法。



## 三、线性代数回顾

### 1、矩阵和向量

![](picture\矩阵定义.png)

矩阵是指由数字组成的矩形阵列，并写在方括号内。

矩阵可以说是二维数组的另一种说法。

矩阵的维数写作矩阵的行数乘以列数。（左边便是一个 $4\times 2$ 的矩阵）

也有这样一种写法：

将左边的矩阵写作 $R^{4\times2}$ ，将该矩阵称为集合 $R^{4\times2}$ 的元素，也就是说， $R^{4\times2}$ 代表所有$4\times2$ 的矩阵的集合。		



![](picture\向量定义.png)

向量是指列数为1的矩阵。

如图 y 表示一个向量，向量的维数由向量的行数决定，y 便是一个4维向量。

$R^4$ 是指一个4维向量的集合。

$y_1$ 表示向量 y 中的第一个元素。

一般有下标从1开始和从0开始两个写法，机器学习的应用中一般用从0开始，本课程默认从1开始。

按惯例来说，用大写字母表示矩阵，用小写字母表示向量。



### 2、加法和标量乘法

![](picture\矩阵的加法.png)

两矩阵相加就是两矩阵对应位置的元素相加，因此，只有维度相同的两矩阵可以相加，维度不同的矩阵相加没有意义。

![](picture\矩阵标量乘法.png)

标量乘法中标量是一个复杂的结构，可能是一个数或者实数。标量乘法就是用这个标量乘矩阵中的每个元素。



### 3、矩阵向量乘法

矩阵和向量相乘：

![](picture\矩阵和向量相乘.png)

![](picture\矩阵向量相乘的应用.png)

在实际应用中，可以如上图这样使用。

有一组数据集（关于房价的）和一个假设函数 $h_{\theta}(x)$ ，那么计算预测值可以如图中这样列为一个矩阵和一个向量相乘。

### 4、矩阵乘法

![](picture\矩阵相乘.png)

假设有一个数据集，三个假设函数，那么计算预测值可以如下图列矩阵：

![](picture\矩阵乘法_2.png)

### 5、矩阵乘法的特性

（1）、矩阵乘法不满足交换律：

​                                            $A \times B \ne B \times A$ 

（2）、矩阵乘法满足结合律：

​                                           $A \times B \times C = (A \times B) \times C = A \times (B \times C)$ 

（3）、矩阵乘以单位矩阵等于原矩阵：

​                                          $A \times I = I \times A = A $      （ $I$ 为单位矩阵）

### 6、逆和转置

（1）、矩阵的逆：

如果 $A$ 是一个 $m \times m$ 的矩阵，如果 $A$ 有逆矩阵，那么

​                                         $AA^{-1} = A^{-1}A = I $ 

（2）、矩阵的转置运算：

​                                        $A = \begin{bmatrix}1&2&0\\3&5&9\end{bmatrix} \quad \quad \quad A^T = \begin{bmatrix}1&3\\2&5\\0&9\end{bmatrix}$     

定义：

假设 $A$ 是一个 $m \times n$ 的矩阵，并设矩阵 $B$ 等于 $A$ 的转置，那么 $B$ 是一个 $n \times m$ 的矩阵，并且 $B_{ij} = A_{ji}$ 。

## 四、多变量线性回归

适用于多变量或者多特征量的情况。

### 1、多功能

![](picture\单变量线性回归模型假设函数.png)

若只有一个变量那我们的假设函数是这样的。

而若有多个特征值，那么假设函数就变为了：

​                                            $h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n$ 

为了方便表示，我们假设了一个并不存在的特征值 $x_0$ ,且 $x_0$ 恒为 $1$ ，那么 $x$ 就可以表示为一个 $n+1$ 维的向量：

​                                                                 $x = \begin{bmatrix}x_0\\x_1\\x_2\\ \vdots\\x_n\end{bmatrix} \in R^{n+1} $ 

同时参数 $\theta$ 也可以表示为一个 $n+1$ 维的向量：

​                                                                 $\theta = \begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\ \vdots\\\theta_n\end{bmatrix} \in R^{n+1} $ 

那么假设函数就变成了：

​                                         $h_\theta(x) = \theta_0x_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n$ 

​                                                      $=\theta^Tx$ 

### 2、多元梯度下降法

#### 2.1、定义

多变量线性回归模型的代价函数定义如下：

![](picture\多变量线性回归模型代价函数定义.png)

和单变量梯度下降法类似，多变量梯度下降法的具体过程如下：

![](picture\多变量梯度下降法过程.png)

（记得为同步更新）

#### 2.2特征缩放

![](picture\多变量梯度下降之特征缩放.png)

特征缩放就是让多个特征值在近似的范围之内，进行梯度下降。

以上图左边两个特征值为例， $x_1$ 的范围是 $0-2000$ ， $x_2$ 的范围是 $1-5$ ，那么不考虑 $\theta_0$ ，代价函数的等高线图如上图左边所示，因为 $x_1,x_2$ 的范围差距极大，所以是一个狭长的椭圆形，那么梯度下降的过程如红线所示，就会左右来回震荡，下降速度缓慢。所以为提高下降速度，将 $x_1$ 的数值除以2000， $x_2$ 的数值除以5，那么等高线图如右图所示，是一个正圆，梯度下降的速度也就较左边快得多。

![](picture\多变量梯度下降之特征缩放特征值范围.png)

把特征值缩放到 -1到1的范围就可以了，不过要求并不十分严格，只要近似就好了，以老师的经验，最大-3到3都是OK的，最小 $-\frac{1}{3} $ 到 $\frac{1}{3}$ 就可以。

除了将特征值除以最大值之外，在特征缩放中有时也会进行一项称为均值归一化的工作。

![](picture\多变量梯度下降之均值归一化.png)

均值归一化就是将特征值 $x_i$ 替换为 $x_i-\mu_i$ ，使你的特征值具有为0的平均值。

一个一般化的公式就是这样：

​                                                                     $\frac{x_i-\mu_i}{s_i} \rightarrow x_i$    

其中 $\mu_i$ 就是 $x_i$ 取值范围的中位数， $s_i$ 就是 $x_i$ 的区间长度（最大值减最小值）。

==注意：由于特征值缩放的目的是减少梯度下降的路径以加快梯度下降的速度，所以特征值缩放的精度并不精确，只要是大体的范围之内便可。==

#### 2.3学习率

![](picture\多变量梯度下降之判断收敛.png)

对于一个多变量的梯度下降算法，我们如何判断结果是否已经收敛呢？

如上图左边所示，代价函数根据迭代次数的变化映射在图中，由图可知，在300次迭代知乎，代价函数下降的值小的可以忽略不计，这时便称已经收敛。

由于不同场景所需的迭代次数差别非常大，所以我们不能仅仅根据迭代次数进行判断，一般是画出右图这样的函数图，根据函数曲线进行判断。

或者如右图所示，有这样的一个自动测试程序，当迭代后下降的小于一个给定的很小很小的值 $\epsilon$ ，便视为已经收敛。

不过由于这样的一个 $\epsilon$ 的值很难确定，所以推荐使用左边的方法，即画出函数曲线。



![](picture\多变量梯度下降之α大小.png)

通过画出代价函数的下降曲线可以很方便的看出代价函数是否在正确运行。

如上图中左上的函数，代价函数的值在不断下降，这时我们一般应该选取一个更小的学习率 $\alpha$ 值，因为导致该图的原因一般为选取的 $\alpha$ 值过大，超过了最小值，并不断左右震荡上升（如右图橙线所示）。

对于左下的函数，也应选取一个更小的函数，数学家已经证明过了，课上没有证明。

==对于一个足够小的 $\alpha$ 值，在所有的迭代中代价函数的值都会下降。不过如果 $\alpha$ 的值过小，代价函数收敛就会很慢。==

![](picture\多变量梯度下降之学习率选取.png)

总结一下：

==如果 $\alpha$ 的值过小，那么代价函数的收敛就会很慢。==

==而如果 $\alpha$ 的值过大，那么代价函数的值在每次迭代中不一定会下降，代价函数也可能不会收敛。==

老师选取 $\alpha$ 的方法，一般是找到一个对于该代价函数来说过大的 $\alpha$ 中最小的一个，以及对于该代价函数来说过小的 $\alpha$ 中最大的一个，然后在该区间内以3的倍数寻找 $\alpha$ ，这样一般能找到一个比较合理的 $\alpha$ 。

### 3、特征和多项式回归

本节要讨论多个不同的特征如何选取合适的特征，以及如何得到不同的算法。当选择了合适的特征后，这些算法往往是非常有效的。

多项式回归能够用线性回归的方法进行拟合非常复杂的函数，甚至是非线性函数。

以预测房价为例：

![](picture\多变量线性回归模型之选择特征.png)

上图的例子给了我们临街宽度 $x_1$ 和纵向深度 $x_2$ 两个特征，但我们很直观的可以知道，和房价更相关的是房屋面积，所以我们可以只用两者的乘积，即房屋面积这一个特征。有时可以通过定义一个新特征，可以得到一个更好的模型。

与选择特征密切相关的一个概念，被称为多项式回归。

![](picture\多变量之多项式模型.png)

比如有如图所示的房价数据集，通过观察数据集你觉得直线并不能很好的拟合数据，因此你很容易就会想到二次模型，但你很快会觉得二次模型不合理，因为二次函数会降下来，但随着房屋面积的增大，房屋的价格并不会降下来。所以可能我们会选择不同的多项式模型，并转而选择一个三次函数。我们用这个三次函数的式子可以得到这样的模型（绿色），这个模型和数据集拟合的更好，因为最终不会下降。

那么我们如何进行拟合呢？

我们将 $size$ 作为 $x_1$ ， $size^2$ 作为 $x_2$ ， $size^3$ 作为 $x_3$ ，这样来进行拟合。

因为这个模型是一个三次模型，所以特征缩放就非常重要。

![](picture\特征选择.png)

事实上我们有很大的余地来选择使用哪些特征，如上图我们可以平方根函数来进行拟合。

### 4、正规方程

正规方程对于某些线性回归问题，可以提供更好的方法来求得参数 $\theta$ 的最优值。

![](picture\正规方程与梯度下降比较.png)

与梯度下降这种一步一步迭代求出 $\theta$ 最优值的解法不同，正规方程提供了一种可以一步直接求出 $\theta$ 最优值的解析法。

![](picture\正规方程过程.png)

以最简单的一种情况为例：

假设 $J(\theta)$ 是一个 $\theta$ 为标量的二次函数，那么如果求最低点的 $\theta$ 值呢？

学过微积分就很容易可以想到，对 $J(\theta)$ 进行求导，求出令 $J'(\theta)=0$ 的那一点的 $\theta$ 值就可以了。

同理，推广到多个特征 $\theta$ 的情况，只要对 $J(\theta)$ 的各个 $\theta$ 求偏导，并使各个偏导等于0，求出该条件下各 $\theta$ 的取值就可以了。

![](picture\正规方程过程2.png)

 其中，将各样本和价格用矩阵表示如上图所示。

根据：

​                                                       $J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$ 

其中： 

​	                                          		$h_{\theta}(x) = \theta^TX =\theta_0x_0+\theta_1x_1+...+\theta_nx_n$  

带入得：

​		                              				$J(\theta)=\frac{1}{2m}(X*\theta-Y)^T(X*\theta-Y)$

​                                                                 $=\frac{1}{2m}(Y^TY-Y^TX\theta-\theta^TX^TY+\theta^TX^TX\theta)$  

对上式进行求导并令 $J'(\theta) =0 $ 

求得                                             ==$\theta=(X^TX)^{-1}X^TY$==

![ ](picture\正规方程过程3.png)

![](picture\正规方程与梯度下降使用范围.png)

比较下梯度下降和正规方程这两种算法：

对于梯度下降算法而言：

1、由于梯度下降需要选择一个合适的学习率 $\alpha$ ，且不一定一次就可以找到合适的 $\alpha$ ，所以需要多次实验测试以找出一个合适的 $\alpha$ 。（缺）

2、梯度下降是一个迭代的算法，所以需要多次迭代。（缺）

3、即使特征值 $n$ 的数量很大也可以运行的很好。（优）

对于正规方程而言：

1、不需要去选择学习率 $\alpha$ 。（优）

2、不需要迭代。（优）

3、需要计算 $(X^TX)^{-1}$ ，时间复杂度大约在 $n$ 的三次方。（缺）

4、当特征值的数量n非常大时，会运行的非常慢。（缺）



以老师的经验，当 $n$ 为100或者1000的规模的时候，当然是选择正规方程算法，不过当到10000的规模的时候，会开始犹豫，可能会倾向于梯度下降算法，但也不绝对。但如果远大于10000的规模，那么基本就会选择梯度下降算法了。



### 5、正规方程在矩阵不可逆的情况下的解决办法

对于 $\theta=(X^TX)^{-1}X^TY$ ，很容易会考虑到一个问题，就是如果 $(X^TX)^{-1}$ 不可逆怎么办。事实上这个发生的情况很少，不过还是需要讨论一下的。

![](picture\正规方程不可逆情况.png)

 $(X^TX)^{-1}$ 不可逆的情况一般要考虑下面两个问题：

1、是否有多余的特征。

图中所给的例子就是既有特征平方，又有特征英尺，而1米等于3.28英尺，这就是多余的特征。

这个其实很好理解，就是矩阵对应成比例导致不满秩呗。

2、特征值过多。

如果特征值大于样本数量那么久很有可能出问题，这种情况可以删去一些特征或者使用正规化。

## 五、Logistic回归

### 1、分类

这节课开始要讨论的是要预测的变量y是一个离散值情况下的分类问题。

![](picture/logistics回归之分类.png)

假设有这样的场景：

1、邮件系统中判断邮件是否是垃圾邮件

2、线上交易系统中判断对方账户是否为欺诈账户

3、判断肿瘤是良性肿瘤还是恶性肿瘤

在上述的场景下输出只会有 0、1这两种离散情况，这就被称为分类问题。

通常情况下将0称为负类，1称为正类，0通常表示没有某样东西的情况，1则表示有某样东西的情况。

现在我们只讨论只有0和1这两种情况的例子，即二分类（二元分类），以后会考虑多分类的情况，即 $y$ 取值0、1、2、3等其他值的情况。 	

![](picture/logistics回归之分类2.png)

给定上图的数据集，即关于肿瘤大小与肿瘤恶性良性的数据集。

首先我们可以用我们学过的线性回归算法进行试验。

开始只有前8个数据集，根据线性回归拟合的假设函数曲线 $h_\theta(x) = \theta^Tx$ 如图粉色线所示，那么我们如何做出预测呢，我们可以将分类器的阈值设为 0.5 ，作垂线如图粉色垂线所示，那么也就是，当 $h_{\theta}(x)$ 的值大于等于0.5时，做出预测 $y=1$ ，即为恶性，反之若小于 0.5 ，则做出预测 $y=0$ ，即为良性。现在看起来这个假设函数对于现有数据集拟合的很好，但当我们多加入一个数据（即最后一个数据），则线性回归的假设函数如图蓝色所示，那么当阈值仍为 0.5 时，作垂线为蓝色垂线出，此时该垂点左边并不都是良性肿瘤，这就是一个很差劲的拟合函数了。

所以对于一个分类问题的数据集进行线性回归有时候效果会好，但通常不是一个好方法。所以通常不推荐将线性回归用在分类问题上。

![](picture/logistics回归之分类3.png)

另外一个问题，如果对分类问题进行线性回归，通常输出的值是要远大于1或者远小于0的，即使所有数据集的值都为0或者1，这就很怪。所以接下来会开发一个名为logistics回归的算法，其特点为算法的输出值均为0和1之间的值，即不会小于0也不会大于1。btw，logistics回归算法是一种分类算法，并不是一种回归算法，名称是因为一些历史遗留问题。

### 2、假设陈述

![](picture/logistics回归之假设陈述.png)

对于线性回归来说，假设函数为 $h_{\theta}(x) = \theta^Tx$ ,而logistics回归是将 $\theta^Tx$ 作为参数，用 $g(z)$ 函数进行计算， $g(z)$ 的公式为 $g(z) = \frac{1}{1+e^{-z}}$ ， $g(z)$ 被称为 Sigmoid函数或者Logistic函数。

 $g(z)$ 函数的函数图像如右下角所示。

用更严谨的数学公式表示下：

![](picture/logistics回归之假设陈述2.png)

像一般情况一样， $x$ 为一个向量， $x_0 = 1$ ， $x_1$ 为肿瘤的大小，而 $h_{\theta}(x)$ 的值则表示肿瘤是恶性肿瘤的概率。

用概率论的方法来表示则为

​                                                       $h_{\theta}(x) = P(y=1|x;\theta)$  

含义为： $y=1 $ ，给定 $x$ 和概率的参数  $\theta$ 时的概率，其中 $\theta$ 的作用是，通过给定的 $\theta$ ，使得样本满足正态时 $\theta x$ 尽可能的大，样本属于负类时 $\theta x$ 尽可能的小。  

### 3、决策界限

![](picture/logistics回归之决策边界.png)

像之前所说的logistic回归 $h_{\theta}(x) = g(\theta^Tx)$ 的曲线如上图所示，而 $h_{\theta}(x)$ 表示的是为恶性肿瘤的概率，但我们要解决的问题是一个分类问题，输出值 $y$ 只能有 0 和 1 这两种值，那么何时取 1 ，何时取 0 呢？

一般来说，当概率大于 0.5 时我们认为其更有可能发生，所以我们就预测，若 $h_{\theta}(x)>0.5$ ，则输出为1，反之则输出为0。

观察 Sigmoid函数 $g(z)$ 曲线可以很容易的看出， $z=0$ 处 $g(z)$ 的取值为0.5，即 z 小于0时， $g(z)<0.5$ ，y = 0；z 大于等于0时， $g(z)>=0.5$ ，y = 1。

又因为 $h_{\theta}(x) = g(\theta^Tx)$ ，所以 $\theta^Tx<0$ 时， $h_{\theta}(x) = g(\theta^Tx)$ < 0.5，y = 0； $\theta^Tx>=0$ 时， $h_{\theta}(x) = g(\theta^Tx)$ >= 0.5，y = 1.

![](picture/logistics回归之决策边界2.png)

假设我们有左上这样一个数据集，同时假设假设函数为 $h_{\theta}(x) = g(\theta_0+\theta_1x_1+\theta_2x_2)$ ,目前还没有提到如何拟合模型中的参数，将在后面的章节提及。但是假设我们已经拟合好了参数，最终选择如下值： $\theta_0=-3,\theta_1 = 1,\theta_2 = 1$ 。

根据上面所论述的，若 $y = 1$ ，则 $h_{\theta}(x) = g(\theta^Tx)>0.5$ ，即 $\theta^Tx>= 0$ ，即 $-3+x_1+x_2>=0$ ，即 $x_1+x_2>=3$ 。而 $x1 + x_2 = 3$ 在图中则为那条洋红色的直线。这条直线则表示 $h_{\theta}(x) = 0.5$ ，这条直线也就是决策边界。

需要澄清一点，决策边界是假设函数的一个属性，它包括参数 $\theta_0,\theta_1,\theta_2$ 。决策边界决定于其参数，不是数据集的属性。

![](picture/logistics回归之决策边界3.png)

让我们看一个更复杂的情况，给出如左上给出的数据集，很明显，我们很难用一条直线将上图的决策界限表示出来。之前在多项式回归和线性回归中，我们提到可以在特征中添加额外的高阶多项式项，在logistic回归中同样可以这样做。

具体来说，假如假设函数如上图所示
$$
h_\theta(x) = g(\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2)
$$
即我们已经添加了两个额外的特征 $x_1^2$ 和 $x_2^2$ 。我们将在后面的章节讲解如何选取合适的参数 $\theta_0,\theta_1,\theta_2,\theta_3,\theta_4$ ，假设我们现在已经做过该操作，并且得出 $\theta_0 = -1,\theta_1 = 0,\theta_2 = 0,\theta_3 = 1,\theta_4 = 1$ ，根据之前的结论，即 $-1+x_1^2+x_2^2 \ge 0$ 时，$h_{\theta}(x) \ge 0.5$ ，$y = 1$ 。

 $x_1^2+x_2^2 = 1$ 即为决策界限。

同理，如果有更多的高阶多项式，就可以得到更复杂的决策边界，而logistic回归可以用来寻找决策边界。

### 4、代价函数

 对这个代价函数的理解是这样的，它是在输出的预测值是h(x)时，而实际标签是y的情况下，我们希望学习算法所付出的代价。

![](picture/logistics回归之代价函数.png)

这一章我们讨论如何选取合适的参数 $\theta$ ，即参数 $\theta$ 的拟合问题。

也就是这样的场景：

我们有 $m$ 个训练集，所以我们有了一个 $n+1$ 维的特征值向量 $x$ ，其中 $x_0 = 1$ ，而输出值 $y$ 只有 0 和 1 两种取值。另外假设函数为
$$
h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
$$


![](picture/logistic回归之代价函数2.png)

在之前的线性回归中我们使用的是这样的一个代价函数：
$$
J(\theta) = \frac{1}{2m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
将其中的 $\frac{1}{2}$ 放到求和之中，变形为：
$$
J(\theta) = \frac{1}{m}\sum\limits^m_{i=1}\frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$
并将求和中的部分提取为另一个函数：
$$
Cost(h_{\theta}(x),y) = \frac{1}{2}(h_\theta(x)-y)^2
$$
其中因为 $h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$ ，是一个非线性函数，其平方之后会变成如左下角图中所示的函数。这种函数被称为非凸函数（与右边凸函数相比较），它存在非常多的局部最优解，这就导致若使用梯度下降算法拟合参数无法保证求出的是我们所期望的最优参数（甚至可能相差极大），于是对于logistic回归算法我们提出另外一种代价函数。

![](picture/logistic回归之代价函数3.png)

这种代价函数为：
$$
Cost(h_{\theta}(x),y) = \begin{cases} 
-log(h_{\theta}(x)) \quad if \quad y = 1\\ 
-log(1-h_{\theta}(x)) \quad if \quad y=0 
\end{cases}
$$
==注：我觉得公式中的 $log$ 应为 $ln$ ，查阅相关资料后也为 $ln$ 。==

这个代价函数乍一看很复杂，但将函数图像画出来之后便非常清晰。

![](picture/logistic回归之代价函数4.png)

当 $y = 1$ 时函数图像如上图所示。

当 $h_\theta(x)$ 越接近 1 时， $Cost$ 函数的值越接近0，即表示为 1 的可能性越大，误差越小，反之 $h_\theta(x)$ 越接近0，则值接近无限大，即表示为 1 的可能性越小，误差越大。

$y = 0$ 时同理。

不过当 $y = 1$ 时，即表示 $h_{\theta}(x) \ge 0.5$ ，所以我认为函数图像中小于0.5的部分其实并不会出现，老师课程中并没有提起这个。 

### 5、简化代价函数与梯度下降

![](picture/logistic回归之简化代价函数.png)

在上一节我们提到过了logistic回归的代价函数为：
$$
Cost(h_{\theta}(x),y) = 
\begin{cases} 
-log(h_{\theta}(x)) \quad if \quad y = 1\\ 
-log(1-h_{\theta}(x)) \quad if \quad y=0 
\end{cases}
$$

由于logistic回归针对的是分类问题，所以 $y$ 的取值只有 0 和 1 ，所以我们可以根据这个特性，将上述的代价函数合并为一个式子：
$$
Cost(h_{\theta},y) = -ylog(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))
$$
这个式子是可以很显然的推出来的，根据 $y$ 的取值。

![](picture/logistic回归之简化代价函数2.png)

于是我们便将logistic回归的代价函数简化为了上述式子，接下来我们要做的就是去寻找合适的参数 $\theta$ ，即拟合参数 $\theta$ ，使得代价函数的值 $J(\theta)$ 的值最小。接下来当给我们一个新的输入 $x$ ，我们就会根据我们的模型（假设函数），输入相应的预测，在这个问题中即该肿瘤是良性的概率。

![](picture/logistic回归之梯度下降.png)

那么如何拟合参数 $\theta$ 呢？那就是使用之前在线性回归模型中所学的梯度下降算法。

其中在梯度下降算法中，在找到最优解之前，会一直循环：
$$
\theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$
其中 
$$
\alpha\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$
具体推倒如下：[^1]
$$
J(\theta) = -\frac{1}{m}[\sum\limits_{i=1}^my^{(i)}ln(h_{\theta}(x^{(i)}))+(1-y^{(i)})ln(1-h_{\theta}(x^{(i)}))] \\
h_{\theta}(x^{(i)}) = \frac{1}{1+e^{-g(x^{(i)})}}  \\ 
g(x^{(i)}) = \theta_0 + \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + \dots + \theta_nx_n^{(i)}  \\ 
[y^{(i)}\ln(h_{\theta}(x^{(i)})) + (1-y^{(i)})\ln(1-h_{\theta}(x^{(i)}))]'\\
= y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}[h_{\theta}(x^{(i)})]' - (1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}[h_{\theta}(x^{(i)})]' \\ 
= [y^{(i)}\frac{1}{h_{\theta}(x^{(i)})} - \frac{1}{1-h_{\theta}(x^{(i)})} + y^{(i)}\frac{1}{1-h_{\theta}(x^{(i)})}][h_{\theta}(x^{(i)})]' \\
\because h_{\theta}(x^{(i)}) = \frac{1}{1+e^{-g(x^{(i)})}}  \\  
\therefore = [y^{(i)}\frac{(1 + e^{-g(x^{(i)})})^2}{e^{-g(x^{(i)})}} - \frac{1 + e^{-g(x^{(i)})}}{e^{-g(x^{(i)})}}][\frac{e^{-g(x^{(i)})}}{(1 + e^{-g(x^{(i)})})^2}g'(x^{(i)})] \\ 
= (y^{(i)} - \frac{1}{1 + e^{-g(x^{(i)})}})g'(x^{(i)}) \\ 
= (y^{(i)} - h_{\theta}(x^{(i)}))g'(x^{(i)}) \\
\because g(x^{(i)}) = \theta_0 + \theta_1x_1^{(i)} + \theta_2x_2^{(i)} + \dots + \theta_nx_n^{(i)} \\
\therefore 当对 \theta_j求偏导时,g'(x^{(i)}) = x_j^{(i)} \\ 
\therefore = (y^{(i)} - h_{\theta}(x^{(i)}))x_j^{(i)} \\ 
\therefore \alpha\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum\limits_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)} （原式得证）
$$


![](picture/logistic回归之梯度下降2.png)



### 6、高级优化















[^1]: 参考自https://blog.csdn.net/ariessurfer/article/details/41310525